{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2f50b9-a691-4093-9356-13caed46df8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Multi-Target Predictions | Feature Selection Via ANOVA or t-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb7221-8eab-45ac-877b-d0b9aa271bbb",
   "metadata": {},
   "source": [
    "#### RE TUNING AND REGULARIZATION APPLIED "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094d2de-090c-44f3-b582-1948b0419d0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f9ddd-187e-4214-a58d-9264c7d63ea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Import Utility Functions\n",
    "from model_utils import classify_skewness, transform_features_for_skewness, transform_targets\n",
    "from model_utils import range_without_outliers, remove_outliers\n",
    "from model_utils import SkewnessTransformer\n",
    "from model_utils import categorical_value_counts_to_df, group_low_frequency_categories\n",
    "from model_utils import get_overfitting_status\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c7ed0-30f3-4144-a29a-34f2739d4469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3eff501-ac2e-4567-8397-8e49fdddd50b",
   "metadata": {},
   "source": [
    "#### Columns To Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6e5d6-958f-4525-8e04-8b124b284796",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_DROP = [\"longitude\", \"latitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2eb86-1a5e-4d74-bf06-d65c1b0fac21",
   "metadata": {},
   "source": [
    "#### Hyperparameters and Other definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1b86c-99b1-40e6-8914-1ca42149bf47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ESTIMATOR_ALPHA: list = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "MODEL_ESTIMATOR_DEPTH: list[int] = [4, 6, 8]\n",
    "MODEL_ESTIMATOR_NUM_LEAVES: list[int] = [31, 50]\n",
    "MODEL_ESTIMATOR_MAX_DEPTH: list[int] = [5, 10, 15]\n",
    "MODEL_ESTIMATOR_N_ESTIMATORS_RF: list[int] = [100, 200]\n",
    "MODEL_ESTIMATOR_N_ESTIMATORS_ADA: list[int] = [50, 100]\n",
    "MODEL_ESTIMATOR_N_ESTIMATORS_BAGGING: list[int] = [10, 50]\n",
    "MODEL_ESTIMATOR_N_NEIGHBORS: list[int] = [5, 10]\n",
    "MODEL_ESTIMATOR_SVR_LINEAR_KERNEL: list[str] = ['linear'] # Restrict kernel to 'linear'\n",
    "MODEL_ESTIMATOR_C_SVR: list = [0.1, 1, 10]\n",
    "MODEL_ESTIMATOR_C_SVR_ADJ: list = [0.01, 0.1, 1]\n",
    "MODEL_CATBOOST_LEARNING_RATE: list = [0.0001, 0.001, 0.01, 0.1]\n",
    "MODEL_CATBOOST_ESTIMATOR_L2_LEAF_REG: list[int] = [1, 3, 5, 10] # Add L2 regularization\n",
    "MODEL_CATBOOST_ESTIMATOR_L2_LEAF_REG_ADJ: list[int] = [3, 5, 7, 10]\n",
    "MODEL_DECISIONTREE_MAX_FEATURES: list = ['sqrt', 'log2', None]  # Feature selection\n",
    "MIN_SAMPLE_LEAF: list[int] = [1, 3, 5, 7]\n",
    "\n",
    "NUM_SIMPLE_IMPUTER: str = \"mean\"\n",
    "CAT_SIMPLE_IMPUTER: str = \"most_frequent\"\n",
    "ONE_HOT_ENCODER_HANDLE_UNKNOWN: str = \"ignore\"\n",
    "POWER_TRANSFORMER_METHOD: str = \"yeo-johnson\"\n",
    "CV: int = 10\n",
    "# GRIDSEARCHCV_SCORING: str = \"neg_mean_absolute_error\"\n",
    "GRIDSEARCHCV_SCORING: str = \"neg_root_mean_squared_error\"\n",
    "RANDOM_STATE: int = 0\n",
    "TRAIN_SIZE: float = 0.7\n",
    "TEST_SIZE: float = 0.2\n",
    "VAL_SIZE: float = 0.1\n",
    "N_REPEATS: int = 10\n",
    "\n",
    "MODEL_SAVE_PATH: str = f\"./checkpoints/trained_multiple_models/\"\n",
    "BEST_MODEL_SUMMARY_CSV_PATH: str = f\"./\"\n",
    "DATASET_VERSION: str = \"v3\"\n",
    "SAVE_SUMMARY_TO_CSV: str = \"True\"\n",
    "\n",
    "# Make dir if it doesn't exist\n",
    "Path(MODEL_SAVE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "# Path(BEST_MODEL_SUMMARY_CSV_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # MLflow experiment setup\n",
    "# URI = \"http://127.0.0.1:5000\"\n",
    "# mlflow.set_experiment(f\"Maz | IPage\")\n",
    "# mlflow.set_tracking_uri(URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da61369-e133-4a4f-ac7a-3b3676ede958",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f9a71-dcca-4932-bd29-77bf88ef02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"../../../data/merged_{DATASET_VERSION}.csv\"\n",
    "# Create a Path object\n",
    "data_file_path = Path(file_path)\n",
    "data = pd.read_csv(data_file_path)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d694d-81c4-45bb-a06e-415800632131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Dataset: {df.head()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354073d-c38e-4862-ad6c-0a19af448587",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = len(df)\n",
    "print(f\"Length of Dataset: {length_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212546f2-92ee-4726-8d15-7ccf8e55ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e1272-2312-400c-b9de-d2c112e1ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b6b52-b57e-4eb1-b7f1-6e8c5d570cc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.isnull().sum().sum())\n",
    "print(\"\\n\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf04718-fa46-44cc-9dd6-2c17e7aef71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated()\n",
    "print(duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2d887-c401-4e92-b819-5be459db97f5",
   "metadata": {},
   "source": [
    "#### Drop Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2d9c0-2eb8-465c-ba6d-38d3cda74a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(COLS_TO_DROP, axis=1, inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436304b8-7c3e-47b4-b04e-1890740ff34e",
   "metadata": {},
   "source": [
    "#### Drop raw longitude and latitude if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ff5ee-4c20-4d7f-8c94-9f6c15dd16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'longitude' in features and 'latitude' in features:\n",
    "#     features.remove(COLS_TO_DROP[0])\n",
    "#     features.remove(COLS_TO_DROP[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61214fa1-398d-4870-9c9d-9f60df3bbbb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Define target variable and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed0c4a-3ae6-4076-8bad-5fa3631b1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"Boron\", \"Zinc\", \"SOC\"]\n",
    "features = [col for col in df.columns if col not in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4c90-7f63-42a9-a716-084107ce62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targe: {targets}\")\n",
    "print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b489ba-50db-4cc3-b360-1f01345c6252",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Identify categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d15207-8f5b-453c-9d1e-f46ae5dd8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df[features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = df[features].select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0623d26-c0ab-4ff3-a584-98e1ff72f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categorical Features:\\n {categorical_features}\\n\")\n",
    "print(f\"Numerical Features:\\n {numerical_features}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d92c0-8f17-4e7f-8d4b-af56cb46a7e2",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d4403-97b5-4664-88f6-1d0748073c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of rows and columns\n",
    "num_targets = len(targets)\n",
    "cols = 3  # Number of columns per row\n",
    "rows = (num_targets + cols - 1) // cols  # Calculate required rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 4))\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through numerical features and plot\n",
    "for i, target in enumerate(targets):\n",
    "    sns.histplot(df[target], kde=True, bins=20, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {target}\")\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(targets), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955d19f-ef27-47e4-9591-c977b3a713f1",
   "metadata": {},
   "source": [
    "##### 1. Check for Skewnes in Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd55723-d243-4a8a-bc2b-03048d63f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness and provide recommendations\n",
    "for target in targets:\n",
    "    skewness = df[target].skew()\n",
    "    skewness_category, recommendation = classify_skewness(skewness)\n",
    "    print(f\"Skewness of '{target}': {skewness:.4f}\")\n",
    "    print(f\"  Skewness Category: {skewness_category}\")\n",
    "    print(f\"  Recommendation: {recommendation}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb551c34-7b51-492e-8302-7bc65fba6d54",
   "metadata": {},
   "source": [
    "#### 1. Analyze Ccategorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4882cd-b16d-426e-b855-f83d8cbcd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Values in 'Area' col:\\n {df['Area'].unique()}\\n\")\n",
    "print(\"*\" * 120)\n",
    "print(f\"Unique Values in 'Soil group' col:\\n {df['Soil group'].unique()}\\n\")\n",
    "print(\"*\" * 120)\n",
    "print(f\"Unique Values in 'Land class' col\\n: {df['Land class'].unique()}\\n\")\n",
    "print(\"*\" * 120)\n",
    "print(f\"Unique Values in 'Soil type' col\\n: {df['Soil type'].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b6f84-0b9b-4e4f-8551-2afa11cb062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts in Categorical Columns of DataFrame\n",
    "counts_df = categorical_value_counts_to_df(df)\n",
    "# Print the resulting DataFrame\n",
    "print(f\"Categorical Value Counts\")\n",
    "print(\"*\" * 24, \"\\n\")\n",
    "# print(counts_df)\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ede2b8-a7c8-4384-bd7e-bd37a9953cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group low frequency categories\n",
    "df = group_low_frequency_categories(df, threshold=5)\n",
    "# Print the resulting DataFrame\n",
    "# print(f\"Grouped Low Frequency Categories\")\n",
    "# print(\"*\" * 32, \"\\n\")\n",
    "# print(modified_df)\n",
    "# modified_df.head(3)\n",
    "\n",
    "# # Check Again | Get the value counts DataFrame\n",
    "# counts_df = categorical_value_counts_to_df(df)\n",
    "# # Print the resulting DataFrame\n",
    "# print(f\"Categorical Value Counts\")\n",
    "# print(\"*\" * 24, \"\\n\")\n",
    "# # print(counts_df)\n",
    "# counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64ba36-5f22-4a83-8757-e51a339d18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # 2 rows and 2 columns\n",
    "\n",
    "# Flatten the axes for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the categorical features and plot\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.countplot(data=df, x=feature, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {feature}\")\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfaa12-759c-4bce-9152-bc670a22af2f",
   "metadata": {},
   "source": [
    "#### 2. Analyze Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6cf15-5570-4c75-933a-9341693321f1",
   "metadata": {},
   "source": [
    "##### 1. Histograms: To visualize the distributions of numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaf745-abc2-4566-920b-ef23909086a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of rows and columns\n",
    "num_features = len(numerical_features)\n",
    "cols = 3  # Number of columns per row\n",
    "rows = (num_features + cols - 1) // cols  # Calculate required rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through numerical features and plot\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.histplot(df[feature], kde=True, bins=20, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {feature}\")\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25938e0-18a5-4953-aa0d-c47e3df1f952",
   "metadata": {},
   "source": [
    "##### 2. Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac094c72-9501-4591-95e6-e7f02e56f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of columns per row\n",
    "cols = 3  # 3 plots in each row\n",
    "num_features = len(numerical_features)\n",
    "rows = (num_features + cols - 1) // cols  # Calculate the required number of rows dynamically\n",
    "\n",
    "# Create subplots dynamically\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 4 * rows))  # Adjust height for rows\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the numerical features and plot boxplots\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.boxplot(x=df[feature], ax=axes[i])\n",
    "    axes[i].set_title(f\"Boxplot of {feature}\", pad=15)  # Add padding for the title\n",
    "\n",
    "# Remove unused subplots (if any)\n",
    "for i in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08271e-99a5-4001-a05e-68649b589c15",
   "metadata": {},
   "source": [
    "##### 3. Display Range of Values in Numerical Features without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96353b9-1c2b-4a8d-a740-ec8e1bd4a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"longitude\", \"latitude\"], axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e222cf-b87c-47e5-8179-f678f215b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the range for each numerical feature\n",
    "print(f\"\\nRange of Values in Each Column (Numerical) without Outliers\")\n",
    "print(\"*\" * 59, \"\\n\")\n",
    "for feature in numerical_features:\n",
    "    min_val, max_val = range_without_outliers(df, feature)\n",
    "    print(f\"{feature}: Min: {min_val}, Max: {max_val}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12253130-18c3-457b-878d-9ba08f1df0c2",
   "metadata": {},
   "source": [
    "##### 4. Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9bbb1-a69a-4fcf-8353-0ae5114aaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically apply the function to each numerical feature\n",
    "for feature in numerical_features:\n",
    "    df = remove_outliers(df, feature)\n",
    "\n",
    "length_df_remove_outliers = len(df)\n",
    "print(f\"Length of Dataset After Removal of Ouliers\")\n",
    "print(\"*\"* 42)\n",
    "print(length_df_remove_outliers)\n",
    "\n",
    "# Print the cleaned dataset\n",
    "print(\"\\nDataset after removing outliers:\")\n",
    "print(\"*\"* 32, \"\\n\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35e0bf-a886-44ae-b3d9-546877aa9b5e",
   "metadata": {},
   "source": [
    "##### 5. Check for Skewness in Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8aeb30-5bd5-4603-b302-a64b0f4a7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[numerical_features].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d659e7-6ebe-4ea0-ac5f-e2b9bf1638cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify skewness\n",
    "def classify_skewness(skewness):\n",
    "    if abs(skewness) > 2:\n",
    "        return \"Highly Skewed\", \"Recommend Log Transformation (if positive) or Yeo-Johnson\"\n",
    "    elif abs(skewness) > 1:\n",
    "        return \"Moderately Skewed\", \"Recommend Yeo-Johnson Transformation\"\n",
    "    elif abs(skewness) > 0.5:\n",
    "        return \"Slightly Skewed\", \"Transformation optional\"\n",
    "    else:\n",
    "        return \"Symmetrical\", \"No transformation needed\"\n",
    "\n",
    "# Check skewness and provide recommendations\n",
    "for feature in numerical_features:\n",
    "    skewness = df[feature].skew()\n",
    "    skewness_category, recommendation = classify_skewness(skewness)\n",
    "    print(f\"Skewness of '{feature}': {skewness:.4f}\")\n",
    "    print(f\"  Skewness Category: {skewness_category}\")\n",
    "    print(f\"  Recommendation: {recommendation}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211d9d0-3a41-44cd-8647-6ba2390c7ec2",
   "metadata": {},
   "source": [
    "#### 3. Analyze Relationships Between Features and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49832427-d9c4-445d-986c-87acbd66ab70",
   "metadata": {},
   "source": [
    "##### 1. Categorical Features vs Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23492-9e4a-4be6-a140-c62e41023fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots dynamically\n",
    "for i, target in enumerate(targets):\n",
    "    # One row, multiple columns\n",
    "    fig, axes = plt.subplots(1, len(categorical_features), figsize=(20, 5))  \n",
    "    \n",
    "    for j, feature in enumerate(categorical_features):\n",
    "        sns.boxplot(data=df, x=feature, y=target, ax=axes[j])\n",
    "        axes[j].set_title(f\"{target} vs {feature}\", pad=10)\n",
    "        axes[j].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Adjust layout for the current row\n",
    "    plt.suptitle(f\"Target: {target}\", fontsize=16)\n",
    "    # Add space for the suptitle\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f24f3e-59cc-4b98-941f-beef39e2e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check unique categories for each feature\n",
    "for feature in categorical_features:\n",
    "    print(f\"{feature}: {df[feature].nunique()} unique categories\")\n",
    "\n",
    "# Step 2: Filter categorical features with more than one unique category\n",
    "valid_categorical_features = [feature for feature in categorical_features if df[feature].nunique() > 1]\n",
    "print(f\"Valid Category Features: {valid_categorical_features}\")\n",
    "\n",
    "# Step 3: Perform ANOVA for each target\n",
    "anova_results_all_targets = {}\n",
    "\n",
    "for target in targets:\n",
    "    anova_results = {}  # To store results for the current target\n",
    "    # print(f\"\\nPerforming ANOVA for target: {target}\")\n",
    "    \n",
    "    for feature in valid_categorical_features:\n",
    "        # Group target values by the categorical feature\n",
    "        groups = [df[df[feature] == category][target].dropna() for category in df[feature].unique()]\n",
    "        \n",
    "        # Perform one-way ANOVA\n",
    "        if len(groups) > 1:  # Ensure at least two groups exist\n",
    "            f_stat, p_value = f_oneway(*groups)\n",
    "            anova_results[feature] = {'F-Statistic': f_stat, 'p-value': p_value}\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    anova_results_df = pd.DataFrame(anova_results).T\n",
    "    anova_results_df['Significant'] = anova_results_df['p-value'] < 0.05\n",
    "    \n",
    "    # Store results for the current target\n",
    "    anova_results_all_targets[target] = anova_results_df\n",
    "\n",
    "# Step 4: Display all results after the loop\n",
    "for target, results_df in anova_results_all_targets.items():\n",
    "    print(f\"\\nANOVA Results for {target}:\")\n",
    "    print(\"*\" * 25)\n",
    "    print(results_df)\n",
    "\n",
    "    # Optionally save to a CSV or Excel file\n",
    "    # results_df.to_csv(f\"anova_results_{target}.csv\", index=True)\n",
    "    # results_df.to_excel(f\"anova_results_{target}.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d363005-df81-4e98-891c-b072e5fd65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features with more than one unique category\n",
    "categorical_features = [feature for feature in categorical_features if df[feature].nunique() > 1]\n",
    "print(f\"Updated Categorical Features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ea235-c2e3-4518-a1d6-f11be4e55bc5",
   "metadata": {},
   "source": [
    "##### 2. Numerical Features vs Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62435916-6516-442c-b658-93cb3773d58c",
   "metadata": {},
   "source": [
    "a. Statistical Tests (ANOVA or t-tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf6f93-ee86-4115-8745-e630da677d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical test for each target\n",
    "statistical_results = {}\n",
    "relevant_features = {}  # Dictionary to store relevant features for each target\n",
    "\n",
    "for target in targets:\n",
    "    # Separate features and target\n",
    "    X = df[numerical_features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Perform F-statistic test\n",
    "    f_stat, p_values = f_regression(X, y)\n",
    "    \n",
    "    # Store results in a DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'Feature': numerical_features,\n",
    "        'F-Statistic': f_stat,\n",
    "        'p-value': p_values\n",
    "    }).sort_values(by='p-value')\n",
    "    \n",
    "    # Save results for the current target\n",
    "    statistical_results[target] = result_df\n",
    "    \n",
    "    # Filter features with p-value < 0.05\n",
    "    relevant_features[target] = result_df[result_df['p-value'] < 0.05]['Feature'].tolist()\n",
    "\n",
    "# Display statistical analysis for each target\n",
    "for target, result_df in statistical_results.items():\n",
    "    print(f\"\\nStatistical Analysis for Target: {target}\")\n",
    "    print(\"*\" * 39)\n",
    "    print(result_df)\n",
    "\n",
    "# Dynamically Split Numerical Features\n",
    "numFeatures_GroupA_Boron = relevant_features.get(\"Boron\", [])\n",
    "numFeatures_GroupA_Zinc = relevant_features.get(\"Zinc\", [])\n",
    "numFeatures_GroupA_SOC = relevant_features.get(\"SOC\", [])\n",
    "\n",
    "# Display relevant features for each target\n",
    "print(\"\\nRelevant Features for Each Target (p-value < 0.05):\")\n",
    "print(\"*\" * 51)\n",
    "\n",
    "if numFeatures_GroupA_Boron:\n",
    "    print(f\"Boron: {numFeatures_GroupA_Boron}\")\n",
    "else:\n",
    "    print(\"Boron: No relevant features found (p-value >= 0.05)\")\n",
    "\n",
    "if numFeatures_GroupA_Zinc:\n",
    "    print(f\"Zinc: {numFeatures_GroupA_Zinc}\")\n",
    "else:\n",
    "    print(\"Zinc: No relevant features found (p-value >= 0.05)\")\n",
    "\n",
    "if numFeatures_GroupA_SOC:\n",
    "    print(f\"SOC: {numFeatures_GroupA_SOC}\")\n",
    "else:\n",
    "    print(\"SOC: No relevant features found (p-value >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a835593-251a-4397-b2e2-ada8ebe356aa",
   "metadata": {},
   "source": [
    "##### b. Handle Multicollinearity for Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da754c8-6fde-4449-8258-e7b424c65222",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flag for regression model\n",
    "REGRESSION_MODEL = True\n",
    "\n",
    "# Function to calculate VIF and handle multicollinearity\n",
    "def handle_multicollinearity_vif(features, df, vif_threshold=15.0):\n",
    "    X = df[features]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = features\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    # Identify features with VIF above the threshold\n",
    "    high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"].tolist()\n",
    "    \n",
    "    # Dynamically create a second group without multicollinear features\n",
    "    group_without_multicollinearity = [f for f in features if f not in high_vif_features]\n",
    "\n",
    "    return group_without_multicollinearity, vif_data\n",
    "\n",
    "# Check and dynamically create groups for each target\n",
    "# Boron\n",
    "if numFeatures_GroupA_Boron:\n",
    "    print(\"\\nHandling Multicollinearity for Boron:\")\n",
    "    print(\"*\" * 38)\n",
    "    numFeatures_GroupB_Boron, vif_boron = handle_multicollinearity_vif(numFeatures_GroupA_Boron, df)\n",
    "    print(f\"Group A (original): {numFeatures_GroupA_Boron}\")\n",
    "    print(f\"Group B (adjusted): {numFeatures_GroupB_Boron}\")\n",
    "    if len(numFeatures_GroupB_Boron) < len(numFeatures_GroupA_Boron):  # Multicollinearity exists\n",
    "        print(\"\\nVIF Data for Boron:\")\n",
    "        print(vif_boron)\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group B for regression.\")\n",
    "        else:\n",
    "            print(\"\\nGroup B created but not needed for tree-based models.\")\n",
    "    else:\n",
    "        print(\"\\nNo multicollinearity found.\")\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group A for regression.\")\n",
    "        else:\n",
    "            print(\"\\nUse Group A for tree-based models.\")\n",
    "\n",
    "# Zinc\n",
    "if numFeatures_GroupA_Zinc:\n",
    "    print(\"\\nHandling Multicollinearity for Zinc:\")\n",
    "    print(\"*\" * 36)\n",
    "    numFeatures_GroupB_Zinc, vif_zinc = handle_multicollinearity_vif(numFeatures_GroupA_Zinc, df)\n",
    "    print(f\"Group A (original): {numFeatures_GroupA_Zinc}\")\n",
    "    print(f\"Group B (adjusted): {numFeatures_GroupB_Zinc}\")\n",
    "    if len(numFeatures_GroupB_Zinc) < len(numFeatures_GroupA_Zinc):  # Multicollinearity exists\n",
    "        print(\"\\nVIF Data for Zinc:\")\n",
    "        print(vif_zinc)\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group B for regression.\")\n",
    "        else:\n",
    "            print(\"\\nGroup B created but not needed for tree-based models.\")\n",
    "    else:\n",
    "        print(\"\\nNo multicollinearity found.\")\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group A for regression.\")\n",
    "        else:\n",
    "            print(\"\\nUse Group A for tree-based models.\")\n",
    "\n",
    "# SOC\n",
    "if numFeatures_GroupA_SOC:\n",
    "    print(\"\\nHandling Multicollinearity for SOC:\")\n",
    "    print(\"*\" * 35)\n",
    "    numFeatures_GroupB_SOC, vif_soc = handle_multicollinearity_vif(numFeatures_GroupA_SOC, df)\n",
    "    print(f\"Group A (original): {numFeatures_GroupA_SOC}\")\n",
    "    print(f\"Group B (adjusted): {numFeatures_GroupB_SOC}\")\n",
    "    if len(numFeatures_GroupB_SOC) < len(numFeatures_GroupA_SOC):  # Multicollinearity exists\n",
    "        print(\"\\nVIF Data for SOC:\")\n",
    "        print(vif_soc)\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group B for regression.\")\n",
    "        else:\n",
    "            print(\"\\nGroup B created but not needed for tree-based models.\")\n",
    "    else:\n",
    "        print(\"\\nNo multicollinearity found.\")\n",
    "        if REGRESSION_MODEL:\n",
    "            print(\"\\nUse Group A for regression.\")\n",
    "        else:\n",
    "            print(\"\\nUse Group A for tree-based models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596501c-b351-4376-a714-38f4e53255fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22017f7d-928d-48ba-a6ae-14b89cb1af94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LinearRegression': {\n",
    "        'model': MultiOutputRegressor(LinearRegression()),\n",
    "        'type': 'regression',\n",
    "        'params': {}  # No significant hyperparameters for Linear Regression\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': MultiOutputRegressor(Lasso()),\n",
    "        'type': 'regression',\n",
    "        'params': {\"model__estimator__alpha\": MODEL_ESTIMATOR_ALPHA}\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model': MultiOutputRegressor(DecisionTreeRegressor()),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__max_depth': MODEL_ESTIMATOR_DEPTH,\n",
    "            'model__estimator__min_samples_leaf': MIN_SAMPLE_LEAF,\n",
    "            'model__estimator__max_features': MODEL_DECISIONTREE_MAX_FEATURES,  # Feature selection\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': MultiOutputRegressor(RandomForestRegressor(random_state=RANDOM_STATE)),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__n_estimators': MODEL_ESTIMATOR_N_ESTIMATORS_RF,\n",
    "            'model__estimator__max_depth': MODEL_ESTIMATOR_MAX_DEPTH,\n",
    "            'model__estimator__min_samples_leaf': MIN_SAMPLE_LEAF,  # Regularization\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': MultiOutputRegressor(AdaBoostRegressor(random_state=RANDOM_STATE)),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__n_estimators': MODEL_ESTIMATOR_N_ESTIMATORS_ADA,\n",
    "        }\n",
    "    },\n",
    "    'Bagging': {\n",
    "        'model': MultiOutputRegressor(BaggingRegressor(random_state=RANDOM_STATE)),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__n_estimators': MODEL_ESTIMATOR_N_ESTIMATORS_BAGGING,\n",
    "        }\n",
    "    },\n",
    "    'KNeighbors': {\n",
    "        'model': MultiOutputRegressor(KNeighborsRegressor()),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__n_neighbors': MODEL_ESTIMATOR_N_NEIGHBORS,\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': MultiOutputRegressor(SVR()),\n",
    "        'type': 'regression',\n",
    "        'params': {\n",
    "        'model__estimator__kernel': MODEL_ESTIMATOR_SVR_LINEAR_KERNEL,  # Restrict kernel to 'linear'\n",
    "        'model__estimator__C': MODEL_ESTIMATOR_C_SVR,  # Include the value 0.1 and others for tuning\n",
    "    }\n",
    "    },\n",
    "    'XGB': {\n",
    "        'model': MultiOutputRegressor(XGBRegressor(random_state=RANDOM_STATE)),\n",
    "        'type': 'tree',\n",
    "        'params': {\n",
    "            'model__estimator__n_estimators': MODEL_ESTIMATOR_N_ESTIMATORS_RF,\n",
    "            'model__estimator__max_depth': MODEL_ESTIMATOR_MAX_DEPTH,\n",
    "            'model__estimator__learning_rate': MODEL_ESTIMATOR_ALPHA,\n",
    "        }\n",
    "    },\n",
    "    'CatBoost': {\n",
    "    'model': MultiOutputRegressor(CatBoostRegressor(verbose=0, random_state=RANDOM_STATE)),\n",
    "    'type': 'tree',\n",
    "    'params': {\n",
    "        'model__estimator__depth': MODEL_ESTIMATOR_DEPTH,  # Example: [4, 6, 8]\n",
    "        'model__estimator__learning_rate': MODEL_CATBOOST_LEARNING_RATE,  # Example: [0.01, 0.1]\n",
    "        'model__estimator__l2_leaf_reg': MODEL_CATBOOST_ESTIMATOR_L2_LEAF_REG,  # Regularization\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14b599-76ee-4e06-80af-f537890051e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, model_info in models.items():\n",
    "#     print(f\"{model_name}: {model_info['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233c797-144d-4e4a-ad40-0e9a79b49e04",
   "metadata": {},
   "source": [
    "#### Define Feature Sets For Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf891b38-9fd8-4527-a6ee-879e6f2990f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets for each group\n",
    "\n",
    "feature_sets = {\n",
    "    'GroupA': {\n",
    "        'Boron': numFeatures_GroupA_Boron + categorical_features,  # Group A for Boron\n",
    "        'Zinc': numFeatures_GroupA_Zinc + categorical_features,    # Group A for Zinc\n",
    "        'SOC': numFeatures_GroupA_SOC + categorical_features       # Group A for SOC\n",
    "    },\n",
    "    'GroupB': {\n",
    "        'Boron': numFeatures_GroupB_Boron + categorical_features,  # Group B for Boron\n",
    "        'Zinc': numFeatures_GroupB_Zinc + categorical_features,    # Group B for Zinc\n",
    "        'SOC': numFeatures_GroupB_SOC + categorical_features       # Group B for SOC\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e64f50-2ff8-4eed-8bc4-1e1d4d67add7",
   "metadata": {},
   "source": [
    "#### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8166d-5b3f-48e4-b0d4-7d06e4ef6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categorical Features: {categorical_features}\")\n",
    "print(f\"Numerical Features: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764edf4-c738-4dd1-8d85-ac68fa63e471",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = df[numerical_features + categorical_features]  # Full feature set\n",
    "y = df[targets]  # Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6812fa-85d4-4ed6-8faf-f7dff6d56b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X Columns: {X.columns}\")\n",
    "print(f\"y Columns: {y.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fc114-58c1-4bfa-9a56-af896f544875",
   "metadata": {},
   "source": [
    "#### Apply Transformations To Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4eb825-257f-42b0-b2d5-0bca804df573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_num_features, num_transformation_report = transform_features_for_skewness(numerical_features, df)\n",
    "# num_transformation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdc6b8-ab00-4f3b-8e90-8c62f3b04acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_num_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f420628-3152-46a0-add6-15530c9c4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update Numerical Features in DataFrameabs\n",
    "# df[numerical_features] = transformed_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c055c-63f4-45c6-8749-12e2dfaa38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Again Check skewness and provide recommendations\n",
    "# for num_feature in numerical_features:\n",
    "#     skewness = df[num_feature].skew()\n",
    "#     skewness_category, recommendation = classify_skewness(skewness)\n",
    "#     print(f\"Skewness of '{num_feature}': {skewness:.4f}\")\n",
    "#     print(f\"  Skewness Category: {skewness_category}\")\n",
    "#     print(f\"  Recommendation: {recommendation}\")\n",
    "#     print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578279c-6311-452f-899a-827fe453883e",
   "metadata": {},
   "source": [
    "#### Apply Transformations To Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d519a7-853b-477a-b695-80d7e9411e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_transformations = {\"Boron\": \"boxcox\"}\n",
    "transformed_targets, target_transformation_report = transform_targets(y, skewness_threshold=0.75, specific_transformations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c56fc-72be-46fb-a6e0-0831a7b56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transformation_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203372b-3f23-441c-b135-7eefe087deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_targets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6a9ee-7900-4f19-88c8-ed76bfd3e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update Targets\n",
    "df[targets] = transformed_targets\n",
    "y = df[targets]\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d694920-f268-414a-98a5-d8876b150e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Check skewness and provide recommendations\n",
    "for target in targets:\n",
    "    skewness = df[target].skew()\n",
    "    skewness_category, recommendation = classify_skewness(skewness)\n",
    "    print(f\"Skewness of '{target}': {skewness:.4f}\")\n",
    "    print(f\"  Skewness Category: {skewness_category}\")\n",
    "    print(f\"  Recommendation: {recommendation}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5297f3a-40c6-4cb3-a16a-1dffbf59390d",
   "metadata": {},
   "source": [
    "#### Split data into train, Val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe02302-99e5-498a-9a61-d623e07d7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numerical_features + categorical_features]  # Full feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ce48c-8335-47f8-96fe-e4736ece8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: Train + Temp (Validation + Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - TRAIN_SIZE), random_state=RANDOM_STATE)\n",
    "\n",
    "# Second split: Validation + Test from Temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(TEST_SIZE / (VAL_SIZE + TEST_SIZE)), random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}, Validation Set: {X_val.shape}, Test Set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10254060-d619-496f-a7f6-d4cd7fdce9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=NUM_SIMPLE_IMPUTER)),\n",
    "        # (\"skewness\", SkewnessTransformer()),  # Apply skewness transformations\n",
    "        (\"scaler\", StandardScaler()),  # Scale features after transformation,\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=CAT_SIMPLE_IMPUTER)),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=ONE_HOT_ENCODER_HANDLE_UNKNOWN)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", numerical_transformer, numerical_features),\n",
    "#         (\"cat\", categorical_transformer, categorical_features),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# pipeline.named_steps['preprocessor'].transformers_[0][1].named_steps['skewness'].skewness_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad7a33-41d8-469a-9542-17691ebf5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categorical Features: {categorical_features}\")\n",
    "print(f\"Numerical Features: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e07d5e-825d-427a-bbf0-dc4dde095ee5",
   "metadata": {},
   "source": [
    "#### Train Models with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856a291-991d-4305-8b92-76a722248baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAGSHUB\n",
    "import dagshub\n",
    "# dagshub.init(repo_owner='mm-mazhar', repo_name='IPAGE', mlflow=True)\n",
    "dagshub.init(repo_owner=\"Omdena\", repo_name=\"IPage\", mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f6cf7-78d4-4d16-9b84-a8cacc7e958a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MLflow experiment setup\n",
    "# URI_LOCAL = \"http://127.0.0.1:5000\"\n",
    "# URI_MAZ = \"https://dagshub.com/mm-mazhar/IPAGE.mlflow\"\n",
    "URI_OMDENA = \"https://dagshub.com/Omdena/IPage.mlflow\"\n",
    "mlflow.set_experiment(\"Maz | Mult-Target | OverFitting Checks and Re-Tuning\")\n",
    "mlflow.set_tracking_uri(URI_OMDENA)\n",
    "\n",
    "# Dictionaries to track the best model and scores for each target\n",
    "best_models = {}\n",
    "best_val_r2_scores = {}  # Track the best validation R² scores\n",
    "best_r2_scores = {}\n",
    "best_model_paths = {}\n",
    "best_features = {}\n",
    "overfitting_metrics = {}\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# Iterate over models and feature sets\n",
    "for model_name, model_info in models.items():\n",
    "    base_model = model_info['model']\n",
    "    model_type = model_info['type']\n",
    "    hyperparameters = model_info.get('params', {})\n",
    "    \n",
    "    # Select feature set based on model type\n",
    "    feature_set_key = 'GroupB' if model_type == 'regression' else 'GroupA'\n",
    "    feature_set = feature_sets[feature_set_key]\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} (Type: {model_type}) using {feature_set_key}... | {counter}\")\n",
    "    print(\"*\" * 65)\n",
    "    \n",
    "    for target, features in feature_set.items():\n",
    "        if target not in best_r2_scores:\n",
    "            best_val_r2_scores[target] = -np.inf\n",
    "            best_r2_scores[target] = -np.inf\n",
    "            best_models[target] = None\n",
    "            best_model_paths[target] = None\n",
    "            best_features[target] = None\n",
    "            overfitting_metrics[target] = \"Unknown\"\n",
    "\n",
    "        selected_numerical_features = [f for f in features if f in numerical_features]\n",
    "        selected_categorical_features = [f for f in features if f in categorical_features]\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numerical_transformer, selected_numerical_features),\n",
    "                (\"cat\", categorical_transformer, selected_categorical_features),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", base_model)\n",
    "        ])\n",
    "\n",
    "        X_train_features = X_train[features]\n",
    "        X_val_features = X_val[features]\n",
    "        X_test_features = X_test[features]\n",
    "\n",
    "        y_train_target_values = y_train[[target]]\n",
    "        y_val_target_values = y_val[[target]]\n",
    "        y_test_target_values = y_test[[target]]\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=hyperparameters,\n",
    "            scoring=GRIDSEARCHCV_SCORING,\n",
    "            cv=CV\n",
    "        )\n",
    "    \n",
    "        with mlflow.start_run(run_name=f\"Maz | {model_name} | {target} | {feature_set_key}\"):\n",
    "            grid_search.fit(X_train_features, y_train_target_values)\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "            val_predictions = best_pipeline.predict(X_val_features)\n",
    "            val_r2 = r2_score(y_val_target_values, val_predictions)\n",
    "\n",
    "            mlflow.log_metric(\"val_r2\", val_r2)\n",
    "\n",
    "            test_predictions = best_pipeline.predict(X_test_features)\n",
    "            mae = mean_absolute_error(y_test_target_values, test_predictions)\n",
    "            mse = mean_squared_error(y_test_target_values, test_predictions)\n",
    "            r2 = r2_score(y_test_target_values, test_predictions)\n",
    "\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "            mlflow.log_metric(\"mse\", mse)\n",
    "            mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "            print(f\"Validation R² Score: {val_r2}\")\n",
    "            print(f\"Test R² Score: {r2}\")\n",
    "            print(f\"MAE: {mae}\")\n",
    "            print(f\"MSE: {mse}\")\n",
    "\n",
    "            # Call the function to get overfitting status\n",
    "            overfitting_status, overfitting_numeric = get_overfitting_status(val_r2, r2)\n",
    "            \n",
    "            # Log numeric overfitting status as a metric\n",
    "            mlflow.log_metric(\"overfitting_status_numeric\", overfitting_numeric)\n",
    "            # Log string overfitting status as a parameter\n",
    "            # mlflow.log_param(\"overfitting_status\", overfitting_status)\n",
    "            # Record in the dictionary\n",
    "            overfitting_metrics[target] = overfitting_status\n",
    "\n",
    "            # If high overfitting, re-tune hyperparameters\n",
    "            if overfitting_status == \"High Overfitting\":\n",
    "                print(f\"    Model {model_name} for target {target} is overfitting. Re-tuning hyperparameters to reduce overfitting...\")\n",
    "                print(\"    \", \"*\" * 112)\n",
    "                \n",
    "                # Adjust hyperparameters dynamically\n",
    "                if model_name in [\"Lasso\", \"ElasticNet\"]:\n",
    "                    hyperparameters[\"model__estimator__alpha\"] = [alpha * 10 for alpha in MODEL_ESTIMATOR_ALPHA]\n",
    "\n",
    "                elif model_name in [\"DecisionTree\", \"RandomForest\"]:\n",
    "                    hyperparameters[\"model__estimator__max_depth\"] = [\n",
    "                        max_depth - 2 for max_depth in MODEL_ESTIMATOR_MAX_DEPTH if max_depth > 3\n",
    "                    ]\n",
    "                    hyperparameters[\"model__estimator__min_samples_leaf\"] = [\n",
    "                        leaf + 2 for leaf in MIN_SAMPLE_LEAF\n",
    "                    ]\n",
    "                elif model_name == \"CatBoost\":\n",
    "                    hyperparameters[\"model__estimator__l2_leaf_reg\"] = MODEL_CATBOOST_ESTIMATOR_L2_LEAF_REG[1:]\n",
    "                        \n",
    "                elif model_name == \"SVR\":\n",
    "                    hyperparameters[\"model__estimator__C\"] = MODEL_ESTIMATOR_C_SVR_ADJ\n",
    "\n",
    "                # Re-run GridSearchCV with updated hyperparameters\n",
    "                print(f\"    Re-running GridSearchCV for {model_name} on target {target}...\")\n",
    "                print(\"    \", \"*\" * 67)\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=hyperparameters,\n",
    "                    scoring=GRIDSEARCHCV_SCORING,\n",
    "                    cv=CV\n",
    "                )\n",
    "                grid_search.fit(X_train_features, y_train_target_values)\n",
    "\n",
    "                # Update best pipeline and metrics after re-tuning\n",
    "                best_pipeline = grid_search.best_estimator_\n",
    "                val_predictions = best_pipeline.predict(X_val_features)\n",
    "                val_r2 = r2_score(y_val_target_values, val_predictions)\n",
    "                \n",
    "                test_predictions = best_pipeline.predict(X_test_features)\n",
    "                mae = mean_absolute_error(y_test_target_values, test_predictions)\n",
    "                mse = mean_squared_error(y_test_target_values, test_predictions)\n",
    "                r2 = r2_score(y_test_target_values, test_predictions)\n",
    "\n",
    "                print(f\"    Updated Validation R²: {val_r2}, Updated Test R²: {r2}\")\n",
    "                print(\"    \", \"*\" * 81)\n",
    "                mlflow.log_metric(\"val_r2\", val_r2)\n",
    "                mlflow.log_metric(\"r2\", r2)\n",
    "                mlflow.log_metric(\"mae\", mae)\n",
    "                mlflow.log_metric(\"mse\", mse)\n",
    "                \n",
    "                print(f\"    Updated Validation R² Score: {val_r2}\")\n",
    "                print(f\"    Updated Test R² Score: {r2}\")\n",
    "                print(f\"    Updated MAE: {mae}\")\n",
    "                print(f\"    Updated MSE: {mse}\")\n",
    "\n",
    "                # Call the function to get overfitting status\n",
    "                overfitting_status, overfitting_numeric = get_overfitting_status(val_r2, r2)\n",
    "                # Log numeric overfitting status as a metric\n",
    "                mlflow.log_metric(\"overfitting_status_numeric\", overfitting_numeric)\n",
    "                # Log string overfitting status as a parameter\n",
    "                # mlflow.log_param(\"overfitting_status\", overfitting_status)\n",
    "                # Record in the dictionary\n",
    "                overfitting_metrics[target] = overfitting_status\n",
    "                \n",
    "            # Log parameters, metrics, and model\n",
    "            logged_params = {\n",
    "                \"model_name\": model_name,\n",
    "                \"target\": target,\n",
    "                \"feature_set\": feature_set_key,\n",
    "                \"features\": \", \".join(map(str, features)),\n",
    "                # \"Validation R²\": val_r2,\n",
    "                # \"Test R²\": r2,\n",
    "                \"Overfitting Status\": overfitting_status,\n",
    "                **grid_search.best_params_,\n",
    "            }\n",
    "            \n",
    "            # Log parameters\n",
    "            for param_name, param_value in logged_params.items():\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "            # Print logged parameters\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Parameters for {model_name} and Target {target}:\")\n",
    "            print(\"-\" * 50)\n",
    "            for param_name, param_value in logged_params.items():\n",
    "                print(f\"    {param_name}: {param_value}\")\n",
    "\n",
    "            # print(\"-\" * 100)\n",
    "\n",
    "            if val_r2 > best_val_r2_scores[target]:\n",
    "                best_val_r2_scores[target] = val_r2\n",
    "\n",
    "            if r2 > best_r2_scores[target]:\n",
    "                best_r2_scores[target] = r2\n",
    "                best_models[target] = best_pipeline\n",
    "                best_features[target] = features\n",
    "                best_model_name = f\"Maz_{model_name}_{target}_{feature_set_key}\"\n",
    "                best_model_path = Path(\n",
    "                    f\"{MODEL_SAVE_PATH}{best_model_name}_r2_{round(r2, 4)}_dataset_{DATASET_VERSION}.pkl\"\n",
    "                )\n",
    "                best_model_paths[target] = best_model_path\n",
    "                joblib.dump(best_pipeline, best_model_path)\n",
    "\n",
    "                input_example = pd.DataFrame([X_test.iloc[0].values], columns=X_test.columns)\n",
    "                mlflow.sklearn.log_model(\n",
    "                    best_pipeline,\n",
    "                    artifact_path=f\"Maz_{model_name}_{target}_{feature_set_key}\",\n",
    "                    input_example=input_example\n",
    "                )\n",
    "                print(f\"New Best Model for {target}: {best_model_name} with Test R²: {r2:.4f}\")\n",
    "                print(f\"Best model for {target} saved to {best_model_path}\")\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305de55-f589-46d3-b144-c91c7934405a",
   "metadata": {},
   "source": [
    "#### Summary of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3ed68-6e0e-469b-b2ed-48e81cd079ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExperimentation Complete!\")\n",
    "print(\"Summary of Best Models:\")\n",
    "print(\"*\" * 80)\n",
    "\n",
    "summary_data = {\n",
    "    \"Target\": [],\n",
    "    \"Best Model\": [],\n",
    "    \"Validation R² Score\": [],\n",
    "    \"Test R² Score\": [],\n",
    "    \"Overfitting Status\": [],\n",
    "    \"Features\": [],\n",
    "    \"Model Path\": []\n",
    "}\n",
    "\n",
    "for target in best_models:\n",
    "    summary_data[\"Target\"].append(target)\n",
    "    summary_data[\"Best Model\"].append(best_models[target].steps[-1][1].__class__.__name__)\n",
    "    summary_data[\"Validation R² Score\"].append(best_val_r2_scores[target])\n",
    "    summary_data[\"Test R² Score\"].append(best_r2_scores[target])\n",
    "    summary_data[\"Overfitting Status\"].append(overfitting_metrics[target])\n",
    "    summary_data[\"Features\"].append(\", \".join(best_features[target]))\n",
    "    summary_data[\"Model Path\"].append(str(best_model_paths[target]) if best_model_paths[target] else \"Not Saved\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "if SAVE_SUMMARY_TO_CSV:\n",
    "    summary_csv_path = Path(\n",
    "        f\"{BEST_MODEL_SUMMARY_CSV_PATH}trained_multiple_models_summary_dataset_{DATASET_VERSION}_reg.csv\"\n",
    "    )\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_csv_path}\")\n",
    "\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ipage",
   "language": "python",
   "name": "env_ipage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
